{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00659fbe-4ba3-49b3-8492-628e15415cdd",
   "metadata": {},
   "source": [
    "## Bag of Words(BoW)\n",
    "\n",
    "- 단어의 순서는 고려하지 않고 단어들의 출현 빈도(frequency) 에만 집중하는 텍스트 데이터의 수치화 방법 \n",
    "- 우선, 각 단어에 고유한 정수 인덱스를 부여 \n",
    "- 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만든다. \n",
    "- scikit-learn의 CountVectorizer를 이용하여 간단히 BoW를 구성할 수 있다. \n",
    "- BoW에는 순서의 개념이 없다... 단순히 출현빈도로만 정리를 한것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b02e5ce9-3714-400b-b3ff-6264ff0d69aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b82711a-6a2f-47a7-9772-2fbcdd001687",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = ['나는 배가 고프다', '내일 점심 뭐먹지','내일 공부해야겠다','점심 먹고 공부 해야지']\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d2dee25-8d99-4f85-ace4-836087f333c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.fit(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91927bf8-4dbf-4c9a-9937-ec3d939a4e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나는': 3, '배가': 7, '고프다': 0, '내일': 4, '점심': 8, '뭐먹지': 6, '공부해야겠다': 2, '먹고': 5, '공부': 1, '해야지': 9}\n"
     ]
    }
   ],
   "source": [
    "print(count_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94543f8d-7521-46d6-bb29-2e3a8ae48478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [text_data[0]]\n",
    "print(count_vectorizer.transform(sentence).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feca8c32-0bf4-45ec-b304-7bbf5171c08a",
   "metadata": {},
   "source": [
    "인덱스별 단어의 카운트만 나오는것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "494715b3-21f3-4929-8218-f726bded0b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "sentence = ['오늘 점심 맛없었어','내일 점심 또 그럴까']\n",
    "print(count_vectorizer.transform(sentence).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d418ab79-b7a6-41e3-9d03-b4793c96c6da",
   "metadata": {},
   "source": [
    "### 문서 단어 행렬(Document-Term Matrix, DTM)\n",
    "https://wikidocs.net/book/2155\n",
    "- 서로 다른 문서들의 BoW들을 결합한 표현 방법  \n",
    "- 다수의 문서에 등장하는 각 단어들의 빈도를 행렬로 표현한 것  \n",
    "- 문서1 : 호기심 많은 고양이  \n",
    "- 문서2 : 꼬리가 긴 고양이  \n",
    "- 문서3 : 호기심 많은 강아지  \n",
    "- 문서4 : 철수는 동물을 좋아해요 \n",
    "|      |강아지|고양이|긴|꼬리가|동물을|많은|좋아해요|철수는|호기심|\n",
    "|------|-----|-----|--|-----|-----|---|-------|-----|-----|\n",
    "|문서1  |  0  |  1  | 0|  0  |  0 | 1  |   0   |  0  |  1  |\n",
    "|문서2  |  0  |  1  |1|  1  |  0 | 0  |   0   |  0  |  0  |\n",
    "|문서3  |  1  |  0  |0|  0  |  0 | 1  |   0   |  0  |  1  |\n",
    "|문서4  |  0  |  0  |0|  0  |  1 | 0  |   1   |  1  |  0  |  \n",
    "- 각 문서에서 등장한 단어의 빈도를 행렬값으로 표시  \n",
    "- 문서들을 서로 비교할 수 있도록 수치화\n",
    "\n",
    "### 문서 단어 행렬의 한계\n",
    "1.희소표현\n",
    "- one-hot-encoding방식의 벡터는 단어 집합의 크기가 벡터의 차원이됨(대부분의 값이 0)\n",
    "- 이는 공간과 계산 리소스의 낭비가 된다. \n",
    "2.단순 빈도수 기반 접근 \n",
    "- 여러 문서에 등장하는 모든 단어에 대해서 빈도수만을 사용한다는 한계가 있다. \n",
    "- 각 문서에는 중요한 단어와 불필요한 단어가 혼재되어있는데....\n",
    "- 따라서, DTM에 불용어와 중요한 단어에 대한 가중치를 부여하는 방법이 필요해진다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ca233-f291-46dd-baa5-ceea1acf1528",
   "metadata": {},
   "source": [
    "## TF-IDF(Term Frequency-Invers Document Frequency)\n",
    "https://wikidocs.net/31698\n",
    "\n",
    "- 단어의 빈도와 역 문서 빈도를 사용하여 DTM내의 각 단어들마다 중요한 정도의 가중치를 부여하는 방법  \n",
    "- 주로 문서의 유사도를 구하는 작업, 검색 시스템에서 검색결과의 중요도를 정하는 작업, 문서 내에서 특정 단어의 중요도를 구하는 작업 등에 쓰임  \n",
    "- TF-IDF 는 TF와 IDF를 곱한 값을 의미  \n",
    "- 문서를 d, 단어를 t, 문서의 총 개수를 n이라고 하면  \n",
    "1) tf(d, t): 특정 문서 d에서의 특정 단어 t의 등장 회수  \n",
    "2) df(t) : 특정 단어 t가 등장한 문서의 수  \n",
    "3) idf(d, t) : df(t)에 반비례하는 수  \n",
    "idf(d, t) = log(n / (1 + df(t)))\n",
    "- 총 문서의 수 n이 급격히 증가하게 되면 IDF의 값이 기하급수적으로 커지는 것을 방지하기 위해 log를 사용  \n",
    "- 특정 단어가 전체 문서에서 등장하지 않게 되는 경우 분모가 0이 되는 것을 방지하기 위해 1을 더함  \n",
    "- TF-IDF는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단  \n",
    "- 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단  \n",
    "- TF-IDF 값이 낮으면 중요도가 낮다고 판단  \n",
    "- 예를 들어 영어에서 the 나 a와 같은 불용어의 경우 모든 문서에 자주 등장하기 때문에 이런 불용어들의 TF-IDF 값은 다른 단어에 비해서 낮아지게 됨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b1d37e2-716a-4cf0-9141-6b7c3b193a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76a6b2b7-3a15-47d7-891a-36db19361d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나는': 3, '배가': 7, '고프다': 0, '내일': 4, '점심': 8, '뭐먹지': 6, '공부해야겠다': 2, '먹고': 5, '공부': 1, '해야지': 9}\n"
     ]
    }
   ],
   "source": [
    "text_data = ['나는 배가 고프다', '내일 점심 뭐먹지','내일 공부해야겠다','점심 먹고 공부 해야지']\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(text_data)\n",
    "print(tfidf_vectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ace6ba20-8eb0-46be-b344-c1bb88839cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.52547275 0.         0.         0.         0.52547275\n",
      "  0.         0.         0.41428875 0.52547275]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [text_data[3]]\n",
    "print(tfidf_vectorizer.transform(sentence).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df9b2c83-8cf6-48a4-b23d-eec7c20cc6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57735027 0.         0.         0.57735027 0.         0.\n",
      "  0.         0.57735027 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.52640543 0.\n",
      "  0.66767854 0.         0.52640543 0.        ]\n",
      " [0.         0.         0.78528828 0.         0.6191303  0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.52547275 0.         0.         0.         0.52547275\n",
      "  0.         0.         0.41428875 0.52547275]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vectorizer.transform(text_data).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a19151-872f-4964-8374-00237af4d9c7",
   "metadata": {},
   "source": [
    "- TF-IDF 값을 사용하는 경우, 단순 횟수를 이용하는것보다 각 단어의 특성을 좀 더 잘 반영할 수 있다. \n",
    "- 모델에 적용할 때도 TfidfVectorizer를 사용하는 것이 일반적으로 더 좋은 결과를 만들어낸다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b75766a-c268-4868-9637-4be58f12d18d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
