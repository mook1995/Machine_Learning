{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39dafc7f-9150-4dce-b265-422658b6c333",
   "metadata": {},
   "source": [
    "## 선형 모델(Linear Models)\n",
    "\n",
    "선형 모델은 100여년 전에 개발되었고, 지난 몇십년 동안 폭넓게 연구되고 현재도 널리 쓰인다. \n",
    "\n",
    "선형 모델은 입력 feature에 대한 선형 함수를 만들어 예측을 수행한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137a990c-3fc1-42fa-a033-0de2700a0b5f",
   "metadata": {},
   "source": [
    "####  회귀의 선형 모델\n",
    "회귀의 경우 선형모델을 위한 일반적인 예측함수는 다음과 같다. \n",
    "- 예측y = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b  \n",
    "\n",
    "여기서 w는 weight b는 bias의 약자이다 \n",
    "\n",
    "x[0] 부터 x[p] 까지는 하나의 데이터 포인트에 대한 feature를 나타내며 (feature의 수는 p+1개 이다)\n",
    "\n",
    "우리는 여기서 모델을 만들것이고 \n",
    "여기서 w와 b 는 모델이 학습할 파라미터이다.\n",
    "그리고 예측 y는 모델이 만들어낸 예측값이다. \n",
    "\n",
    "만약에 특성이 하나인 데이터 셋이라면 식은 다음과 같다. \n",
    "예측 y= w[0]* x[0] + b\n",
    "여기서 w[0]는 기울기이고 b는 y축과 만나는 절편이다. \n",
    "\n",
    "만약에 feature가 많아지면 w는 각 feature에 해당하는 기울기를 모두 가지게 된다. \n",
    "\n",
    "여기서 기울기는 x값이 y값에 얼마나 영향을 주냐 하는 정도를 나타내는것으로 볼수 있다. \n",
    "\n",
    "아무튼, 다른게 생각하면 예측값은 입력 feature에 w의 각 가중치(음수도 가능)를 곱해서 더한 가중치 합으로 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82af3121-47e2-46fc-9a61-0f51dad4ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn \n",
    "import matplotlib.pyplot as plt\n",
    "mglearn.plots.plot_linear_regression_wave()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94698e09-be8d-4d88-8b9d-25dd4fd4697f",
   "metadata": {},
   "source": [
    "회귀를 위한 선형 모델은 feature가 하나일 때는 직선, 두개일 떈 평면이 되며, 더 높은 차원에서는 초평면(hyperplace)이 되는 회귀모델의 특징을 가진다. \n",
    "\n",
    "초평면 부터는 그림으로 그리지 못한다. \n",
    "2차원을 나타내기 위해 3차원이 필요하고 1차원직선을 나타내기 위해 2차원이 필요하기 때문이다. \n",
    "\n",
    "이 직선과 KNeighborsRegression을 사용하여 만든 이전 그림의 선과 비교해보면 직선을 사용한 예측이 더 제약이 많아 보인다. \n",
    "\n",
    "즉, 데이터의 상세정보를 모두 잃어버린것처럼 보이는데 이 말도 어느정도 사실이다. \n",
    "\n",
    "따라서 target y가 feature들의 선형 조합이라는 것은 매우 과한(떄론 비현실적인) 가정이다.\n",
    "\n",
    "하지만 1차원 데이터셋만 놓고 봐서 생긴 편견일 수 있다. \n",
    "\n",
    "feature가ㅣ 많은 데이터셋이라면 선형 모델은 매울 훌륭한 성능을 낼 수 있다. \n",
    "\n",
    "그래서 회귀를 위한 선형모델은 다양하게 존재하며 이 모델들은 훈련 데이터로부터 모델 파라미터 w와 b를 학습하는 방법과 \n",
    "모델의 복잡도를 제어하는 방법에서 차이가 난다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf1c7c9-316f-4c47-9ec9-7cd0c87a8a4a",
   "metadata": {},
   "source": [
    "##  선형 회귀(최소제곱법) : Ordinary least squares \n",
    "\n",
    "    선형회귀(linear regression) 또는 최소 제곱법(ordinary least squares)은 가장 간단하고 오래된 회귀용 선형 알고리즘이다 \n",
    "    선형 회귀는 예측과 훈련 세트에 있는 target y사이의 평균제곱오차(mean squared error)를 최소화 하는 파라미터 w와 b를 찾는다. \n",
    "    평균 제곱오차는 예측값과 target 값의 차이를 제곱하여 더한 후에 샘플의 수로 나눈것이다. \n",
    "    최소 제곱법은 추가 매개변수가 없는 것이 장점이지만, 모델의 복잡도를 제어할 방법또한 없다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2dc3b51-80ca-4618-9373-272a2a37131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = mglearn.datasets.make_wave(n_samples=60)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "lr = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd5aa57-b82a-4c71-ba52-a578ef5cb7d3",
   "metadata": {},
   "source": [
    "기울기 파라미터 w는 가중치(weight) 또는 계수(coefficient)라고 하며 lr 객체의 coef_속성에 저장되어있고 , 편향(bias)또는 \n",
    "절편(intercept) 파라미터 b는 intercept_속성에 저장되어있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f15dc867-9484-4597-9314-ed0ce0bed09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr.coef_: [0.39390555]\n",
      "lr.intercept_: -0.031804343026759746\n"
     ]
    }
   ],
   "source": [
    "print(\"lr.coef_:\", lr.coef_)\n",
    "print(\"lr.intercept_:\", lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781be299-6530-4fa3-b9f1-da389d0cf818",
   "metadata": {},
   "source": [
    "intercept_속성은 항상 실수값 하나지만, coef_속성은 각 입력 feature에 하나씩 대응되는 Numpy 배열이다. \n",
    "wave 데이터셋에는 입력 feature가 하나뿐이므로 lr.coef_도 원소를 하나만 가지고 있는것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37eacbdb-10bf-4ac7-ac8c-77cb5aca7ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.67\n",
      "Test set score: 0.66\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5da5a8a-01c8-42e6-93a9-af86e385a3eb",
   "metadata": {},
   "source": [
    "하지만 훈련 세트와 테스트 세트의 점수가 매우 비슷, 즉, 과대적합이 아니라 과소적합인 상태를 의미한다. \n",
    "1차원 데이터셋에서는 모델이 매우 단순하므로 과대적합을 걱정할 필요가 없다. \n",
    "그러나 feature가 많은 고차원 데이터셋에서는 선형 모델의 성능이 매우 높아져서 과대적합될 가능성이 높아진다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bce4e94-60de-412d-8c4f-3699d6027111",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mglearn.datasets.load_extended_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "lr = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cd8f69e-32c5-4073-a3d3-8ab49d273d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.95\n",
      "Test set score: 0.61\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503e8933-978e-46ac-ba7f-d3285d9fdb46",
   "metadata": {},
   "source": [
    "feature수를 많이 늘렸더니 과대적합이 발생한것을 볼 수 있다. \n",
    "이런 train과 test의 성능의 차이는 모델이 과대적합되었다는 확실한 신호이므로 복잡도를 제어할 수 있는 모델을 사용해야한다. \n",
    "\n",
    "때문에 이러한 문제를 제어하기 위해 수학자들이 개발한것이 새로운 선형회귀 알고리즘들이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ccb9b1-6d4c-48e2-b284-47c27ad5b397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
