{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c15311-330e-4fb5-9b64-c04b23597f2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## word2vec 을 이용한 모델 \n",
    "\n",
    "- word2vec 은 단어로 표현된 리스트를 입력값으로 넣어야 한다. \n",
    "- 전처리된 텍스트를 불러온 후 각 단어들의 리스트로 나눠야 한다. \n",
    "- wor2vec 작동원리 설명:  https://wikidocs.net/22660\n",
    "- 코사인 거리를 구하기 위한 벡터 연산 방법 : https://m.blog.naver.com/PostView.naver?blogId=wndrlf2003&logNo=221540829487&isFromSearchAddView=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fd85f7c-5d7e-42e3-868b-f92185d3a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re \n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d80777b6-a230-47fe-9fbb-9dd2233e526a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stuff going moment mj started listening music ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>classic war worlds timothy hines entertaining ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>film starts manager nicholas bell giving welco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>must assumed praised film greatest filmed oper...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>superbly trashy wondrously unpretentious explo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  stuff going moment mj started listening music ...          1\n",
       "1  classic war worlds timothy hines entertaining ...          1\n",
       "2  film starts manager nicholas bell giving welco...          0\n",
       "3  must assumed praised film greatest filmed oper...          0\n",
       "4  superbly trashy wondrously unpretentious explo...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "DATA_OUT_PATH = './data_out/'\n",
    "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
    "\n",
    "train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b96319-edd8-4bbd-b7e2-6e2ef94baaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = list(train_data['review'])\n",
    "sentiments = list(train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "737a046b-2efd-492e-bbb4-494f5c9068b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7515fbb0-afac-416d-bf2d-8e6e4efb830b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalker maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj feeling towards press also obvious message drugs bad kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond mj overheard plans nah joe pesci character ranted wanted people know supplying drugs etc dunno maybe hates mj music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another think people stay away try give wholesome message ironically mj bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter\n"
     ]
    }
   ],
   "source": [
    "print(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a3ae9ea-1032-434d-a95c-1795cd8b447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for review in reviews:\n",
    "    sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6ef87d6-5034-4429-9114-5c12cf9d8648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "504f8f6c-3e7e-4142-bb3f-d8f421336e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'watched', 'moonwalker', 'maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'really', 'cool', 'eighties', 'maybe', 'make', 'mind', 'whether', 'guilty', 'innocent', 'moonwalker', 'part', 'biography', 'part', 'feature', 'film', 'remember', 'going', 'see', 'cinema', 'originally', 'released', 'subtle', 'messages', 'mj', 'feeling', 'towards', 'press', 'also', 'obvious', 'message', 'drugs', 'bad', 'kay', 'visually', 'impressive', 'course', 'michael', 'jackson', 'unless', 'remotely', 'like', 'mj', 'anyway', 'going', 'hate', 'find', 'boring', 'may', 'call', 'mj', 'egotist', 'consenting', 'making', 'movie', 'mj', 'fans', 'would', 'say', 'made', 'fans', 'true', 'really', 'nice', 'actual', 'feature', 'film', 'bit', 'finally', 'starts', 'minutes', 'excluding', 'smooth', 'criminal', 'sequence', 'joe', 'pesci', 'convincing', 'psychopathic', 'powerful', 'drug', 'lord', 'wants', 'mj', 'dead', 'bad', 'beyond', 'mj', 'overheard', 'plans', 'nah', 'joe', 'pesci', 'character', 'ranted', 'wanted', 'people', 'know', 'supplying', 'drugs', 'etc', 'dunno', 'maybe', 'hates', 'mj', 'music', 'lots', 'cool', 'things', 'like', 'mj', 'turning', 'car', 'robot', 'whole', 'speed', 'demon', 'sequence', 'also', 'director', 'must', 'patience', 'saint', 'came', 'filming', 'kiddy', 'bad', 'sequence', 'usually', 'directors', 'hate', 'working', 'one', 'kid', 'let', 'alone', 'whole', 'bunch', 'performing', 'complex', 'dance', 'scene', 'bottom', 'line', 'movie', 'people', 'like', 'mj', 'one', 'level', 'another', 'think', 'people', 'stay', 'away', 'try', 'give', 'wholesome', 'message', 'ironically', 'mj', 'bestest', 'buddy', 'movie', 'girl', 'michael', 'jackson', 'truly', 'one', 'talented', 'people', 'ever', 'grace', 'planet', 'guilty', 'well', 'attention', 'gave', 'subject', 'hmmm', 'well', 'know', 'people', 'different', 'behind', 'closed', 'doors', 'know', 'fact', 'either', 'extremely', 'nice', 'stupid', 'guy', 'one', 'sickest', 'liars', 'hope', 'latter']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])\n",
    "# 2차원 리스트로 만들어짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4194319e-5fea-456f-b103-e9741efa9529",
   "metadata": {
    "tags": []
   },
   "source": [
    "## word2Vec 학습을 위한 gensim 설치\n",
    "-(lstm-env) $ conda install -c anaconda gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4e386ee-48a6-4c9c-a143-22fb53676d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300 \n",
    "min_word_count = 40 \n",
    "num_workers = 4 \n",
    "context = 10\n",
    "downsampling = 1e-3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d43cb8-edcd-429f-a7fe-443b738fa3ae",
   "metadata": {},
   "source": [
    "- num_features : 각 단어에 대해 임베딩된 벡터의 차원 지정(feature 수)\n",
    "- min_word_count : 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습하지 않기 위해 설정  \n",
    "- num_workers : 모델 학습 시 학습을 위한 쓰레드 수 지정(기본값 3)  \n",
    "                내부적으로 학습을 좀 더 빠르게 진행시키기 위함임\n",
    "- context : word2vec 을 수행하기 위한 컨텍스트 윈도우 사이즈 지정  \n",
    "a. Maximum distance between the current and predicted word within a sentence.  \n",
    "b. 기준 단어의 앞뒤에 존재하는 단어들로 기준 단어를 예측하게 되는데(sg=0, CBOW-Continuous Bag of Words)  \n",
    "c. 이 때 기준 단어에서 앞뒤 얼마나 떨어져 있는 단어까지 고려하는가를 결정\n",
    "- downsampling : word2vec 학습을 수행할 때 빠른 학습을 위해 정답 단어 레이블에 대한 다운샘플링 비율을 지정  \n",
    "a. 보통 0.001이 좋은 성능을 낸다고 알려짐  \n",
    "b. 0.001 값을 threshold 값으로 보고, 이 값보다 빈도수가 높은 단어들은 무작위로(랜덤) 다운샘플링 됨  \n",
    "c. 빈도수가 높은 단어는 다운샘플링하여 가끔 학습(랜덤하게 무시)하고 빈도수가 낮은 단어는 출현 족족 학습하는 효과 . 실험적으로 1e-3이 제일 효과가 좋다고 하므로 이것을 그대로 사용할것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e140b487-8ca2-4a85-bb0e-1876e3a18592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "#기본적으로 모든 언어마다 logging이라는게 있는데 그 수준(level)을 어느 수준으로 할것이냐를 정해줄 수 있다. \n",
    "#실제 현장에서는 logging을 자주 쓰는데 \n",
    "#그러한 logging을 깔끔하게 하기 위한 툴이 logging 툴이다. \n",
    "logging.basicConfig(format = '%(asctime)s : %(levelname)s : %(message)s', level = logging.INFO )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fee3bb9-7368-4ad4-8a7c-8cbee10cd939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-13 23:53:58,556 : INFO : collecting all words and their counts\n",
      "2021-10-13 23:53:58,558 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-10-13 23:53:58,763 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n",
      "2021-10-13 23:53:58,982 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n",
      "2021-10-13 23:53:59,094 : INFO : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n",
      "2021-10-13 23:53:59,094 : INFO : Loading a fresh vocabulary\n",
      "2021-10-13 23:53:59,136 : INFO : effective_min_count=40 retains 8160 unique words (11% of original 74065, drops 65905)\n",
      "2021-10-13 23:53:59,137 : INFO : effective_min_count=40 leaves 2627273 word corpus (87% of original 2988089, drops 360816)\n",
      "2021-10-13 23:53:59,159 : INFO : deleting the raw counts dictionary of 74065 items\n",
      "2021-10-13 23:53:59,161 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2021-10-13 23:53:59,162 : INFO : downsampling leaves estimated 2494384 word corpus (94.9% of prior 2627273)\n",
      "2021-10-13 23:53:59,183 : INFO : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n",
      "2021-10-13 23:53:59,184 : INFO : resetting layer weights\n",
      "2021-10-13 23:54:00,633 : INFO : training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2021-10-13 23:54:01,645 : INFO : EPOCH 1 - PROGRESS: at 43.41% examples, 1083960 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-13 23:54:02,651 : INFO : EPOCH 1 - PROGRESS: at 87.21% examples, 1085329 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-13 23:54:02,921 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-13 23:54:02,927 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-13 23:54:02,941 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-13 23:54:02,945 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-13 23:54:02,945 : INFO : EPOCH - 1 : training on 2988089 raw words (2494369 effective words) took 2.3s, 1080843 effective words/s\n",
      "2021-10-13 23:54:03,948 : INFO : EPOCH 2 - PROGRESS: at 46.80% examples, 1174360 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-13 23:54:04,960 : INFO : EPOCH 2 - PROGRESS: at 93.47% examples, 1160390 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-13 23:54:05,084 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-13 23:54:05,089 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-13 23:54:05,092 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-13 23:54:05,094 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-13 23:54:05,095 : INFO : EPOCH - 2 : training on 2988089 raw words (2494339 effective words) took 2.1s, 1161833 effective words/s\n",
      "2021-10-13 23:54:06,103 : INFO : EPOCH 3 - PROGRESS: at 43.10% examples, 1077934 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-13 23:54:07,128 : INFO : EPOCH 3 - PROGRESS: at 79.76% examples, 982852 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-13 23:54:07,520 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-13 23:54:07,527 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-13 23:54:07,533 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-13 23:54:07,542 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-13 23:54:07,542 : INFO : EPOCH - 3 : training on 2988089 raw words (2494546 effective words) took 2.4s, 1020239 effective words/s\n",
      "2021-10-13 23:54:08,551 : INFO : EPOCH 4 - PROGRESS: at 43.41% examples, 1085554 words/s, in_qsize 8, out_qsize 0\n",
      "2021-10-13 23:54:09,554 : INFO : EPOCH 4 - PROGRESS: at 90.34% examples, 1125088 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-13 23:54:09,727 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-13 23:54:09,733 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-13 23:54:09,741 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-13 23:54:09,750 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-13 23:54:09,750 : INFO : EPOCH - 4 : training on 2988089 raw words (2494263 effective words) took 2.2s, 1131154 effective words/s\n",
      "2021-10-13 23:54:10,769 : INFO : EPOCH 5 - PROGRESS: at 48.74% examples, 1204520 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-13 23:54:11,771 : INFO : EPOCH 5 - PROGRESS: at 88.58% examples, 1099032 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-13 23:54:12,080 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-13 23:54:12,086 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-13 23:54:12,090 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-13 23:54:12,104 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-13 23:54:12,105 : INFO : EPOCH - 5 : training on 2988089 raw words (2494578 effective words) took 2.4s, 1060421 effective words/s\n",
      "2021-10-13 23:54:12,106 : INFO : training on a 14940445 raw words (12472095 effective words) took 11.5s, 1087284 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, workers = num_workers, size = num_features,\n",
    "                          min_count = min_word_count, window = context, sample= downsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed8547-45cf-4530-91b1-a2837d16201c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject\n",
    "\n",
    "- 우리가 텐서플로우 떄문에 numpy를 1.19로 설치했는데 이러한 에러는 numpy 1.20을 설치해야 한다는군요 ....\n",
    "- 파이썬 3.8 이상을 쓰면 numpy를 1.20대를 써야 한답니다. \n",
    "- 그니까 파이썬 3.7 대를 쓰면 괜찮았을 텐데 ...\n",
    "- 이게 1.20대를 못쓴게 keras에서 제공하는 imbedding layer때문에 그랬죠 ...\n",
    "- 아무튼 python 버전 다운 하고 다시 받아서 gensim 이랑 embeding layer 돌아가면 되는거겠죠?\n",
    "- 다시 합시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9510e655-7d14-4ffc-8325-08f5617bc8fb",
   "metadata": {},
   "source": [
    "## SOLUTION1 .....새로운 가상환경 만들기\n",
    "- $(base) conda create -n python37-env python=3.7\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf92190-8959-4664-a5a1-487041efa31b",
   "metadata": {},
   "source": [
    "## SOLUTION2\n",
    "- (lstm-env) $ conda uninstall gensim \n",
    "\n",
    "- (lstm-env) $ conda install -c anaconda gensim = 3.8.3\n",
    "\n",
    "- 해결됨!! , 현재 gensim이 4.대가 깔려있는데 잠깐 사이에 버전업이 된것이었고 그게 numpy 1.9랑호환이 안된것임 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2450a25-d964-4446-bc1a-90d0ececc3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-08 11:35:50,724 : INFO : collecting all words and their counts\n",
      "2021-10-08 11:35:50,725 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-10-08 11:35:50,925 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n",
      "2021-10-08 11:35:51,120 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n",
      "2021-10-08 11:35:51,228 : INFO : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n",
      "2021-10-08 11:35:51,229 : INFO : Loading a fresh vocabulary\n",
      "2021-10-08 11:35:51,269 : INFO : effective_min_count=40 retains 8160 unique words (11% of original 74065, drops 65905)\n",
      "2021-10-08 11:35:51,269 : INFO : effective_min_count=40 leaves 2627273 word corpus (87% of original 2988089, drops 360816)\n",
      "2021-10-08 11:35:51,290 : INFO : deleting the raw counts dictionary of 74065 items\n",
      "2021-10-08 11:35:51,293 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2021-10-08 11:35:51,294 : INFO : downsampling leaves estimated 2494384 word corpus (94.9% of prior 2627273)\n",
      "2021-10-08 11:35:51,315 : INFO : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n",
      "2021-10-08 11:35:51,316 : INFO : resetting layer weights\n",
      "2021-10-08 11:35:52,790 : INFO : training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2021-10-08 11:35:53,795 : INFO : EPOCH 1 - PROGRESS: at 44.81% examples, 1123004 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 11:35:54,806 : INFO : EPOCH 1 - PROGRESS: at 92.75% examples, 1151309 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 11:35:54,929 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 11:35:54,933 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 11:35:54,947 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 11:35:54,949 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 11:35:54,950 : INFO : EPOCH - 1 : training on 2988089 raw words (2494630 effective words) took 2.2s, 1156285 effective words/s\n",
      "2021-10-08 11:35:55,960 : INFO : EPOCH 2 - PROGRESS: at 44.81% examples, 1116502 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 11:35:56,975 : INFO : EPOCH 2 - PROGRESS: at 86.58% examples, 1072818 words/s, in_qsize 6, out_qsize 1\n",
      "2021-10-08 11:35:57,258 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 11:35:57,269 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 11:35:57,274 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 11:35:57,286 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 11:35:57,287 : INFO : EPOCH - 2 : training on 2988089 raw words (2494171 effective words) took 2.3s, 1068675 effective words/s\n",
      "2021-10-08 11:35:58,294 : INFO : EPOCH 3 - PROGRESS: at 44.11% examples, 1103717 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 11:35:59,302 : INFO : EPOCH 3 - PROGRESS: at 90.03% examples, 1118950 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 11:35:59,502 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 11:35:59,506 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 11:35:59,508 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 11:35:59,518 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 11:35:59,520 : INFO : EPOCH - 3 : training on 2988089 raw words (2493972 effective words) took 2.2s, 1118366 effective words/s\n",
      "2021-10-08 11:36:00,531 : INFO : EPOCH 4 - PROGRESS: at 45.16% examples, 1125687 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 11:36:01,537 : INFO : EPOCH 4 - PROGRESS: at 92.40% examples, 1147600 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 11:36:01,687 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 11:36:01,695 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 11:36:01,696 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 11:36:01,705 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 11:36:01,706 : INFO : EPOCH - 4 : training on 2988089 raw words (2494435 effective words) took 2.2s, 1143586 effective words/s\n",
      "2021-10-08 11:36:02,710 : INFO : EPOCH 5 - PROGRESS: at 45.44% examples, 1140198 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 11:36:03,716 : INFO : EPOCH 5 - PROGRESS: at 93.84% examples, 1168158 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 11:36:03,825 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 11:36:03,826 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 11:36:03,827 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 11:36:03,832 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 11:36:03,832 : INFO : EPOCH - 5 : training on 2988089 raw words (2494466 effective words) took 2.1s, 1174894 effective words/s\n",
      "2021-10-08 11:36:03,833 : INFO : training on a 14940445 raw words (12471674 effective words) took 11.0s, 1129367 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, workers = num_workers, size = num_features,\n",
    "                          min_count = min_word_count, window = context, sample= downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b14615b0-2a8c-4aee-aa4c-d7911abb7e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-08 11:36:10,447 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2021-10-08 11:36:10,447 : INFO : not storing attribute vectors_norm\n",
      "2021-10-08 11:36:10,448 : INFO : not storing attribute cum_table\n",
      "2021-10-08 11:36:10,579 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd44734b-6913-492e-8c58-6392fca243de",
   "metadata": {},
   "source": [
    "- 위에서 만든 word2vec 모델을 활용하여 선형 회귀 모델을 학습시켜본다. \n",
    "- 각 리뷰를 같은 형태의 입력값으로 만들어야 한다. \n",
    "- 리뷰마다 단어의 수가 모두 다르므로 입력 값을 하나의 형태로 만들어야 한다. \n",
    "- 가장 단순한 방법으로, 문장에 있는 모든 단어의 벡터값에 대해 평균을 내서 리뷰 하나당 하나의 벡터로 만드는 방법을 사용할 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37479535-7023-409b-ace0-0b5f1031670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(words, model, num_features):\n",
    "    #word로 하나의 리뷰가 들어옴 , num_features에는 300차원의 feature를 말함\n",
    "    feature_vector = np.zeros((num_features), dtype=np.float32)\n",
    "    #제로 벡터 생성 \n",
    "    \n",
    "    num_words = 0\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #index2word는 wv(word2vec)의 단어사전임\n",
    "    \n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            #단어사전에 존재하면 count를 하나 증가시키고 \n",
    "            num_words += 1\n",
    "            feature_vector = np.add(feature_vector, model[w])\n",
    "            #제로로 만든 벡터에다가 해당 단어의 벡터를 꺼내서 넣어준다는것이다. \n",
    "            \n",
    "    feature_vector = np.divide(feature_vector, num_words)\n",
    "    #그리고 그 모든 단어의 벡터값을 다 더해서 평균을 내는것임 \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dfa4e6-1141-40f3-883e-ba4bdab746d9",
   "metadata": {},
   "source": [
    "- words : 단어의 모음인 하나의 리뷰가 들어감\n",
    "- model : word2vec 모델\n",
    "- num_features : word2vec 으로 임베딩할 때 정했던 벡터의 차원 수  \n",
    "- 결극 하나의 문장에 등장하는 사전에 등록된 단어들의 벡터값의 평균을 구함  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c527aaf-efa0-456d-abc5-8b9985c2f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(reviews, model, num_features):\n",
    "    dataset = list()\n",
    "    \n",
    "    for s in reviews:\n",
    "        dataset.append(get_features(s, model, num_features))\n",
    "        \n",
    "    reviewFeatureVecs = np.stack(dataset)\n",
    "    \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f436ba0-e3da-4c0a-afd9-014b1c2b8114",
   "metadata": {},
   "source": [
    "- review : 전체 리뷰 데이터  \n",
    "- model : 학습시킨 모델  \n",
    "- num_features : word2vec 임베딩 시 정했던 벡터의 차원 수  \n",
    "- np.stack(dataset, axis=0) 은 row 로 데이터를 쌓으면서 numpy 배열을 만든다는 의미  \n",
    "- 이렇게 하여 row가 전체 샘플 수 만큼, column 은 feature의 차원 수가 됨  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f28d7ecb-a6ab-43c6-96a5-598594190ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fplc8\\AppData\\Local\\Temp/ipykernel_4704/4139925217.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  feature_vector = np.add(feature_vector, model[w])\n"
     ]
    }
   ],
   "source": [
    "train_data_vecs = get_dataset(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4584a0ba-70fe-43f4-a60f-6be898d3f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_data_vecs\n",
    "y = np.array(sentiments)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TEST_SPLIT = 0.2 \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = TEST_SPLIT,\n",
    "                                                    random_state= RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c142042-6306-4bfa-9acd-5f5409f0ae16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\lstm-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lgs = LogisticRegression(class_weight='balanced')\n",
    "lgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "454bbae1-4592-46cf-aef7-c57b296f7029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.867200\n"
     ]
    }
   ],
   "source": [
    "predicted = lgs.predict(X_test)\n",
    "print('Accuracy : %f' %lgs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c6d8117-5ef0-4486-be5a-1e37114cc657",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CLEAN_DATA = 'test_clean.csv'\n",
    "\n",
    "test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)\n",
    "test_review = list(test_data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eb1b5ff-4f29-4a89-a74b-ce2fc826e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = list()\n",
    "for review in test_review:\n",
    "    test_sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa032be3-5416-4578-b17c-982c318f0903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fplc8\\AppData\\Local\\Temp/ipykernel_8076/3998795672.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  feature_vectort = np.add(feature_vector, model[w])\n"
     ]
    }
   ],
   "source": [
    "test_data_vecs = get_dataset(test_sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3387b5dc-1f75-41c3-a910-139da88d65fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = lgs.predict(test_data_vecs)\n",
    "\n",
    "ids = list(test_data['id'])\n",
    "answer_dataset = pd.DataFrame({'id': ids, 'sentiment':test_predicted})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ac9487a-7189-46bf-bf6a-9ba4a08a8495",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "    \n",
    "answer_dataset.to_csv(DATA_OUT_PATH + 'lgs_answer1.csv', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "829fd14e-5cce-4ee5-a21c-60190327e1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930dc087-ab17-482a-ba10-f1fe1776baf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
